{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.13782133])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = True\n",
    "not a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Input:\n",
    "    \n",
    "    def __init__(self, input_shape, activation):\n",
    "        self.input_shape = input_shape\n",
    "        self.activation = activation\n",
    "        \n",
    "    def info(self):\n",
    "        # we return name, dict. info\n",
    "        return 'input', {'shape': self.input_shape[0], 'activation function': self.activation}\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.info()\n",
    "\n",
    "# or will Input inherit from Dense?\n",
    "class Dense(Input):\n",
    "    \n",
    "    # let this be static\n",
    "    hidden_num = 0\n",
    "    \n",
    "    def __init__(self, neurons, activation):\n",
    "        self.neurons = neurons\n",
    "        self.activation = activation\n",
    "        Dense.hidden_num += 1\n",
    "    \n",
    "    def get_neurons(self):\n",
    "        return self.neurons\n",
    "    \n",
    "    def get_activation(self):\n",
    "        return self.activation\n",
    "    \n",
    "    #@staticmethod\n",
    "    def info(self):\n",
    "        # mimick Input\n",
    "        return f'hidden {self.hidden_num}', {\n",
    "            'shape': self.neurons, 'activation function': self.activation\n",
    "        }\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{(self.neurons, self.activation)}\"\n",
    "        \n",
    "# how to make this a parent of Dense\n",
    "class NeuralNet(Dense):\n",
    "    \n",
    "    def __init__(self, _type='artificial'):\n",
    "        self.layers = dict()\n",
    "        self.weights = np.array([])\n",
    "        self.biases = np.array([])\n",
    "        self.NODES = np.array([])  # len is simply number of neurons\n",
    "        self.activation = list()\n",
    "        self.input_shape = None\n",
    "        self.output_shape = None\n",
    "        \n",
    "        self.w_visited = 0\n",
    "        self.n_visited = 0\n",
    "        \n",
    "        self.curr_layer = 1\n",
    "    \n",
    "    # initialize weights and biases\n",
    "    def add_layers(self, layers):\n",
    "        self.input_shape = (layers[0].get_neurons(),)\n",
    "        for L in range(1, len(layers)):\n",
    "            # number of neurons for both layers\n",
    "            lay1_n, activate = str.split(repr(layers[L-1]))\n",
    "            lay1_n = eval(lay1_n[1:-1])\n",
    "            activate = activate[1:-2]\n",
    "            lay2_n = layers[L].get_neurons()\n",
    "            curr_weights = np.random.randn(lay1_n*lay2_n,)\n",
    "            curr_biases = np.random.randint(-10, 10, size=lay2_n)\n",
    "            \n",
    "            # adding weights of layer to array of weights\n",
    "            self.weights = np.concatenate((self.weights, curr_weights))\n",
    "            self.biases = np.concatenate((self.biases, curr_biases))\n",
    "            self.activation.append(activate)\n",
    "            \n",
    "            # caching the information\n",
    "            self.layers[f\"layer #{L}\"] = {\n",
    "                'shape': (layers[L-1].get_neurons(),),\n",
    "                'activation function': activate,\n",
    "            }\n",
    "        \n",
    "        self.output_shape = (layers[-1].get_neurons(),)\n",
    "        self.activation.append(layers[-1].get_activation())\n",
    "        self.layers[\"output\"] = {\n",
    "            'shape': self.output_shape,\n",
    "            'activation function': self.activation[-1]\n",
    "        }\n",
    "        \n",
    "        self.NODES = np.random.randn(self.input_shape[0])\n",
    "    \n",
    "    def compile_model(self, loss, optimizer):\n",
    "        pass\n",
    "    \n",
    "    # return table with all info; reference self.layers\n",
    "    def info(self):\n",
    "        return \"\"\n",
    "    \n",
    "    def sigmoid(self, z, derivative=False):\n",
    "        if derivative:\n",
    "            pass\n",
    "        return 1 / (1 + np.e**-z)\n",
    "    \n",
    "    def activate(self, val, kind, forward=True):\n",
    "        if kind.lower()=='relu':\n",
    "            return max(0, val)\n",
    "        elif kind.lower()=='sigmoid':\n",
    "            return self.sigmoid(val, not forward)\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "    \n",
    "    \n",
    "    # x values at layer l\n",
    "    def forward(self):\n",
    "        # x_layer will be np.array([x1, x2, x3,..., xn]), where n is the number of neurons\n",
    "        \n",
    "        try:\n",
    "            lay = self.curr_layer\n",
    "            lays = list(self.layers.keys())\n",
    "            shape = self.layers[lays[lay]]['shape'][0]\n",
    "        except IndexError:\n",
    "            print(\"Can't go forwards anymore\")\n",
    "        else:\n",
    "            # number of weights needed for this forward pass\n",
    "            num_w = self.layers[lays[lay-1]]['shape'][0] * shape\n",
    "            W = self.weights[self.w_visited : self.w_visited+num_w]\n",
    "            print(W)\n",
    "            self.w_visited += num_w\n",
    "            print(num_w)\n",
    "            \n",
    "            # storing values of subsequent nodes\n",
    "            new_nodes = []\n",
    "            for node in range(shape):\n",
    "                # record number of previous layer's nodes\n",
    "                n_prev = int(num_w/shape)\n",
    "                # weighted sum of previous inputs plus bias\n",
    "                next_node = np.sum(\n",
    "                    W[node*n_prev : node*n_prev + n_prev] * self.NODES[self.n_visited : self.n_visited+n_prev] + 0\n",
    "                )\n",
    "                # transform in some activation function\n",
    "                act_func = self.activation[lay]\n",
    "                next_node = self.activate(next_node, kind=act_func, forward=True)\n",
    "                new_nodes.append(next_node)\n",
    "                \n",
    "            self.n_visited += n_prev\n",
    "            self.NODES = np.concatenate((self.NODES, np.array(new_nodes)))\n",
    "            \n",
    "        \n",
    "        self.curr_layer += 1\n",
    "    \n",
    "    # start by specifying neuron values with x\n",
    "    def fit(self, x, y, epochs=1, metrics=[], *args, **kwargs):\n",
    "        assert type(x)==np.ndarray and type(y)==np.ndarray\n",
    "        if x.shape != self.input_shape:\n",
    "            raise ValueException(\"X shaped does not match that of input layer's\")\n",
    "        if y.shape != self.output_shape:\n",
    "            raise ValueException(\"Y shaped does not match that of output layer's\")\n",
    "        \n",
    "        \n",
    "        #### Start with caching input values\n",
    "        X = np.array([x[:self.input_shape]])\n",
    "        \n",
    "        print(X)\n",
    "        \n",
    "        \n",
    "#         for epoch in range(epochs):\n",
    "#             # compute forward(x[...], y[...], epoch)\n",
    "            \n",
    "#             pass\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer #1': {'shape': (60,), 'activation function': 'relu'},\n",
       " 'layer #2': {'shape': (120,), 'activation function': 'relu'},\n",
       " 'output': {'shape': (2,), 'activation function': 'sigmoid'}}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = NeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [Input(input_shape=(60,), activation='relu'),\n",
    "          Dense(neurons=120, activation='relu'),\n",
    "          Dense(neurons=40, activation='relu'),\n",
    "          Dense(neurons=2, activation='sigmoid')]\n",
    "ann.add_layers(\n",
    "    layers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60,)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.NODES.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12080,)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -5.,   6.,   2.,   2.,  -3.,  -5.,  -8.,   1.,   5.,  -4.,   8.,\n",
       "         9., -10., -10.,   4.,   3.,  -3.,  -8.,   6.,   9.,   7.,   3.,\n",
       "       -10.,   5.,  -6.,  -7.,   0.,   0.,   2.,   5.,   5.,  -6.,  -7.,\n",
       "        -9.,   0., -10.,   8., -10.,  -7.,   1.,  -5.,  -2.,  -4.,  -7.,\n",
       "         2.,  -5.,   6.,   3.,  -9.,  -1.,   8.,  -5.,   9.,   6.,  -3.,\n",
       "         2.,  -3.,   3.,  -7.,  -4.,  -8.,   7.,  -9.,  -2.,  -7.,   6.,\n",
       "        -8.,   7.,   2.,   9.,  -2.,  -3.,  -6.,   8.,  -7.,  -3.,   3.,\n",
       "         3.,  -1.,   7.,   7.,   7.,  -5.,  -3.,   1.,   8.,   4.,  -1.,\n",
       "        -6.,   8.,   5.,   0.,  -3.,  -3.,  -3.,   7.,  -5.,  -6.,   9.,\n",
       "         6.,   5., -10.,   7.,  -5.,   9.,   2.,   7.,   2.,   6.,   5.,\n",
       "         3.,   7.,   8.,  -4.,   0.,   5.,   6.,  -4.,   3.,  -9.,  -8.,\n",
       "        -7.,   2.,  -1.,  -9.,   4.,   9.,   8.,   0., -10.,  -3.,  -7.,\n",
       "        -4.,  -3.,   6.,   5.,   2.,  -1.,   6.,   6.,   4.,   0.,  -9.,\n",
       "         7.,  -7.,   4.,  -5.,  -1.,  -8.,   0.,   5.,   8.,  -9.,  -9.,\n",
       "         1.,  -9.,   2.,   1.,  -6.,   7.,  -3.,  -2.])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't go forwards anymore\n"
     ]
    }
   ],
   "source": [
    "ann.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(222,)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.NODES.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.36352085 -2.06617258]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "ann.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4000000000000004"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_5 = np.array([0.3, 0.2, 0.9])\n",
    "x_5 = np.array([1, `2, 3])\n",
    "np.sum(w_5*x_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.41724708  2.91896738]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "ann.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.12452795,  0.61054005, -0.6994094 ,  0.28667038, -1.16375719,\n",
       "       -3.51658164])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.NODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.80343933])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.weights[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.,  5., -7.,  7.,  9., -2.,  2., -2.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test this with actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.fetch_openml(name=\"mnist_784\", version=1,\n",
    "                             return_X_y=True, as_frame=False)\n",
    "X/=255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = NeuralNet()\n",
    "\n",
    "ann.add_layers(\n",
    "    [Dense(neurons=X[0].shape[0], activation='relu'),\n",
    "    Dense(neurons=1000, activation='relu'),\n",
    "    Dense(neurons=np.unique(y).shape[0], activation='sigmoid')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.compile_model(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure shapes are good\n",
    "ann.fit(\n",
    "    X, y, metrics=['accuracy', 'loss'], epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.predict(X[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
